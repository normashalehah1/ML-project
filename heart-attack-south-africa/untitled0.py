# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AIvivDIjNrwm36DwBLIajfbUKVLjM3gb
"""

from google.colab import drive
drive.mount('/content/drive/')

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('/content/drive/MyDrive/Belajar analisis data w python/project min-2 februari 2025/Heart Attack in youth vs Adult in South Africa/heart_attack_south_africa.csv')
df

"""##1. Data Understanding"""

print('data shape :', df.shape)

#terdapat 100000 baris data dan 20 kolom feature

df.head()

"""Dari data di atas ada beberapa yang akan kita muat menjadi 0 dan 1.
1. Smoking_Status
2. Alcohol_Intake
3. Physical_Activity
4. Diabetes_Status
5. Family_History_Heart_Disease
6. Diet_Quality
7. Stress_Level
8. Heart_Attack_History
9. Medication_Usage

10. Gender
"""

df.info()

df.describe()

print(df['Heart_Attack_Outcome'].value_counts())

"""##2. Data Cleaning"""

print("\nMissing values per fitur:")
print(df.isnull().sum())

# Cek data duplikat
print(df.duplicated().sum())

# Hapus kolom 'Patient_ID' secara permanen
df.drop(columns=['Patient_ID'], inplace=True)

# Pastikan kolom telah dihapus
print(df.head())  # Cek apakah 'Patient_ID' sudah tidak ada

"""##3. Exploratory Data Analysis(EDA)

###1. Cek data outlier
"""

# Fitur numerik yang akan dicek outlier
num_features = ["Age", "Cholesterol_Level", "Blood_Pressure_Systolic",
                "Blood_Pressure_Diastolic", "Obesity_Index",
                "Triglycerides_Level", "LDL_Level", "HDL_Level"]

# Plot boxplot untuk setiap fitur numerik
plt.figure(figsize=(15, 8))
df[num_features].boxplot()
plt.xticks(rotation=45)
plt.title("Boxplot Outlier Detection")
plt.show()

# Boxplot satu persatu feature
sns.boxplot(df['Blood_Pressure_Diastolic'])
plt.show()

# Deteksi outlier menggunakan IQR
Q1 = df[num_features].quantile(0.25)
Q3 = df[num_features].quantile(0.75)
IQR = Q3 - Q1

# Tentukan batas bawah dan batas atas
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Cek jumlah outlier di tiap fitur
outliers = ((df[num_features] < lower_bound) | (df[num_features] > upper_bound)).sum()
print("Jumlah outlier per fitur:\n", outliers)

"""Dari semua di atas kesimpulannya bahwa tidak memiliki outlier

###2. Cek Distribusi Fitur
"""

# Distribusi fitur numerik
num_features = data.select_dtypes(include=[np.number])
plt.figure(figsize=(14, 10))
for i, column in enumerate(num_features.columns, 1):
    plt.subplot(3, 4, i)
    sns.histplot(data[column], bins=30, kde=True, color='blue')
    plt.title(f'Distribusi {column}')
plt.tight_layout()
plt.show()

# Distribusi fitur kategorikal Grafik ini memperlihatkan frekuensi setiap kategori dalam fitur kategorikal, membantu untuk memahami seberapa sering masing-masing kategori muncul dalam dataset.
categorical_cols = ["Gender", "Smoking_Status", "Alcohol_Intake", "Physical_Activity",
                    "Diabetes_Status", "Family_History_Heart_Disease", "Diet_Quality", "Stress_Level",
                    "Heart_Attack_History", "Medication_Usage", "Heart_Attack_Outcome"]

plt.figure(figsize=(14, 12))
for i, col in enumerate(categorical_cols):
    plt.subplot(4, 3, i+1)
    sns.countplot(x=df[col], palette="viridis")
    plt.title(f"Distribusi {col}")
    plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""###3. Cek korelasi antar fitur(heatmap)

Disni melakukan label encoder dulu agar heatmapnya akurat
"""

from sklearn.preprocessing import LabelEncoder
# Buat instance LabelEncoder
label_encoder = LabelEncoder()

# List kolom kategorikal yang perlu di-encode
categorical_columns = ['Gender','Smoking_Status', 'Alcohol_Intake', 'Physical_Activity', 'Diabetes_Status', 'Family_History_Heart_Disease', 'Diet_Quality', 'Stress_Level', 'Heart_Attack_History', 'Medication_Usage']

# Encode kolom kategorikal
for column in categorical_columns:
    df[column] = label_encoder.fit_transform(data[column])

# Tampilkan DataFrame untuk memastikan encoding telah diterapkan
df.head()

# Heatmap korelasi untuk fitur numerik
plt.figure(figsize=(12, 10))
correlation_matrix = num_features.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Heatmap Korelasi')
plt.show()

df.describe()

"""##4. Feature Engineering

###1. Min-Max Scaler
Min-Max Scaler adalah salah satu metode normalisasi yang digunakan untuk mengubah skala fitur ke rentang tertentu, biasanya antara 0 dan 1. Ini berguna untuk memastikan bahwa semua fitur memiliki skala yang sama sehingga algoritma machine learning bisa bekerja lebih optimal.
"""

from sklearn.preprocessing import MinMaxScaler

# Buat objek Min-Max Scaler
scaler = MinMaxScaler()

# Pilih hanya fitur numerik yang perlu dinormalisasi (tidak termasuk fitur biner)
numerical_features = [
    "Age", "Cholesterol_Level", "Blood_Pressure_Systolic", "Blood_Pressure_Diastolic",
    "Physical_Activity", "Obesity_Index", "Triglycerides_Level", "LDL_Level", "HDL_Level"
]

# Terapkan Min-Max Scaler ke fitur numerik
df_scaled = df.copy()
df_scaled[numerical_features] = scaler.fit_transform(df[numerical_features])

# Tampilkan hasil normalisasi
print(df_scaled.head())


#Feature Selection
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel

# Misal X adalah fitur dan y adalah label target X = df.drop(columns=['Heart_Attack_Outcome']), y = df['Heart_Attack_Outcome']
# Pisahkan fitur (X) dan target (y)
X = df.drop(columns=['Heart_Attack_Outcome'])  # Semua fitur kecuali target
y = df['Heart_Attack_Outcome']  # Target klasifikasi (0 atau 1)

# 1Ô∏è‚É£ Buat model Random Forest
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X, y)

# 2Ô∏è‚É£ Ambil feature importance
feature_importances = model.feature_importances_
feature_names = X.columns

# 3Ô∏è‚É£ Pilih fitur yang importance-nya lebih besar dari median
selector = SelectFromModel(model, threshold="median", prefit=True)
X_selected = selector.transform(X)

# 4Ô∏è‚É£ Lihat fitur yang dipilih
selected_features = X.columns[selector.get_support()]
print("Fitur yang dipilih:", selected_features.tolist())

# 5Ô∏è‚É£ Visualisasi fitur penting
sorted_idx = np.argsort(feature_importances)[::-1]
plt.figure(figsize=(10, 6))
plt.bar(range(len(feature_importances)), feature_importances[sorted_idx], align="center")
plt.xticks(range(len(feature_importances)), np.array(feature_names)[sorted_idx], rotation=90)
plt.xlabel("Fitur")
plt.ylabel("Importance Score")
plt.title("Feature Importance dari Random Forest")
plt.show()

from sklearn.model_selection import train_test_split

# Split data jadi training & testing (80%-20%)
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)

# üìå 4Ô∏è‚É£ Scaling (Hanya untuk model yang butuh)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# üìå 5Ô∏è‚É£ Definisikan Model
models = {
    "K-Nearest Neighbors (KNN)": KNeighborsClassifier(n_neighbors=5),
    "Decision Tree (DT)": DecisionTreeClassifier(max_depth=10, min_samples_split=5, random_state=42),
    "Random Forest (RF)": RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42),
    "Support Vector Machine (SVM)": SVC(kernel='linear', probability=True),
    "Naive Bayes (NB)": GaussianNB()
}


# üìå 6Ô∏è‚É£ Fungsi Evaluasi Model
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()
    results = {
        'Confusion Matrix': cm,
        'True Positive (TP)': tp,
        'False Positive (FP)': fp,
        'False Negative (FN)': fn,
        'True Negative (TN)': tn,
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1-Score': f1_score(y_test, y_pred)
    }
    return results

# üìå 7Ô∏è‚É£ Latih & Evaluasi Model
results = {}
for name, model in models.items():
    if name in ["K-Nearest Neighbors (KNN)", "Support Vector Machine (SVM)", "Naive Bayes (NB)"]:
        model.fit(X_train_scaled, y_train)
        results[name] = evaluate_model(model, X_test_scaled, y_test)
    else:
        model.fit(X_train, y_train)
        results[name] = evaluate_model(model, X_test, y_test)
        
# üìå 8Ô∏è‚É£ Buat DataFrame untuk Ringkasan Hasil
summary_df = pd.DataFrame([
    {
        'Model': name,
        'Accuracy': metrics['Accuracy'],
        'Precision': metrics['Precision'],
        'Recall': metrics['Recall'],
        'F1-Score': metrics['F1-Score']
    }
    for name, metrics in results.items()
])

# üìå 9Ô∏è‚É£ Tampilkan Hasil
print(summary_df)
